# Redactor

## Abstract
Information leakage is becoming a critical problem as various information becomes publicly available by mistake, and machine learning models train on that data to provide services. As a result, one's private information could easily be memorized by such trained models. Unfortunately, deleting information is out of the question as the data is already exposed to the Web or third-party platforms. Moreover, we cannot necessarily control the model trainings by other parties either. In this setting, we study the problem of {\em targeted disinformation} where the goal is to lower the unknown model's accuracy and confidence of predictions on a specific target (e.g., a person's profile) for the purpose of privacy. We focus on end-to-end training where models are trained from scratch without transfer learning and use structured data, which is the common data type to store personal information. While our problem is related to data privacy and defenses against exploratory attacks, our techniques are inspired by targeted data poisoning attacks with some key differences. We show that our problem is best solved by finding the closest points to the target in the input space that will be labeled as a different class. Since we do not control the labeling process, we instead conservatively estimate the labels probabilistically by combining decision boundaries of multiple classifiers using data programming techniques. We also propose techniques for making the disinformation realistic. Our experiments show that a probabilistic decision boundary can be a good proxy for labelers, and that our approach outperforms other targeted poisoning methods when using end-to-end training on real datasets. 

<img src = "https://user-images.githubusercontent.com/62869983/147636983-0513b786-2661-4664-85cf-957f5f10b354.png" width="70%" height="70%">
